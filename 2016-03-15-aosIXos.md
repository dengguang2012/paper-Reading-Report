---
layout: post
title: IX
categories:
- technology
tags:
- technology
- 
---

##一个受保护的高吞吐量低延迟网络数据处理操作系统 阅读报告

### 背景

传统观点竞争网络需求，譬如小消息高速率分组和毫秒级别的延迟是在内核之外用户级别网络栈最好的解决方案。IX实在保存已有的内核保护下的已经有的优点情况下。
ix使用硬件虚拟化来区分内核网络处理的管理和调度。数据平面架构建立在本地零拷贝的API之上，通过指定硬件线程和网络队列数据到数据面为带宽和延迟进行了优化，通过减少相关交通和多核同步来处理有界批量完成的数据。
这些要求在今天的数据中心并不少见（如关键值存储）每一个环节的懈怠延迟效应的多个请求的分布式计算模型。，IX相比linux在吞吐量和点对点的延迟上有很好的提升。

首先，在如今的数据中心应用比如搜索、社交网络，电子商务平台对于系统软件都有很高的要求。单独一个应用包含了很多的软件服务，部署在不同的服务器上，使得对于高性能io流的堆栈有很高的需求，对于小消息有很高的包速率，
对于请求毫秒级要求的反应时间，另外对于安全模式也有很高的要求，但是传统系统往往是矛盾的，一些系统忽略了内核在用户层次上面实现网络栈，但性能不好。IX是一个高吞吐量，低延迟，安全而资源效率高的操作系统。
IX数据中心允许网络栈对带宽和延迟进行优化，它的架构建立在从高性能中间件的经验指导下，如防火墙，负载均衡器，以及软件路由器。IX分离控制平面，其负责系统配置和粗粒度资源应用程序之间的配置，
从运行网络协议栈和应用的dataplanes运行数据平面内核，并在不同的应用保护水平和控制平面中隔离数据平面。在我们的实施中，控制平面是完整的Linux内核和运行在保护状态的dataplane，在专用硬件线程基于库的操作系统中。
大规模化数据中心应用带来了独特的挑战：系统软件和网络堆栈：微秒延迟，要启用大量经常互动的服务，而不会影响用户经历整体延迟，由于每个用户的请求往往涉及数百台服务器，我们还必须考虑的RPC请求跨数据中心的延迟。

高分组速率：这些请求很多时候构成一个的各种服务之间的答复
数据中心的应用程序是相当小的。在Facebook的memcached的服务，绝大多数请求使用密钥长度小于50字节，并且每个节点可以扩展到服务每秒百万的请求。

IX 数据层次允许网络协议栈，对带宽和延迟进行优化。它是围绕一个本地，零拷贝的接口，支持处理的数据包的有界的批处理完成的设计。每个dataplane执行所有的网络处理阶段在dataplane内核的一批数据包，跟着在用户模式下面相关的应用处理。

这种方法摊销的API开销和提高改善指令和数据局部特性。作者基于负载自适应设置批量大小。IX dataplane还优化了多核的可扩展性。网络适配器（NIC）进行流的一致性散列的传入流量到不同的队列。每个dataplane实例只控制一组这些队列和运行网络协议栈和
一个单一的应用程序没有同步或相干交通需要共同的案例操作过程中。IX API与POSIX API，其设计是由交换规则引导。然而，这libix用户级库包括基于API，类似于流行的libevent库事件，提供范围广泛的现有应用程序的兼容性。

我们比较了IX与Linux 3.16.1和MTCP 的TCP/IP dataplane，一个当前最先进的用户级的TCP / IP协议栈。在使用短消息10GbE实验，IX优于Linux和MTCP的分别10和1.9倍数吞吐量。IX进一步扩展使用一个4x10gbe配置，使用单个多核套接字，卸下的两IX服务器单向延迟是5.7µS，这是比标准的Linux内核好处了4倍数和
比MTCP好出一个数量级，作为延迟吞吐量的权衡。对memcached进行了评测，广泛部署的键值存储，显示，IX改善了Linux的3.6×在吞吐量方面在一个给定的百分第九十九的延迟绑定，因为它可以减少核时，由于基本上是网络处理，从75%到10% Linux与IX。
IX表明，通过重新审视网络API，利用现代网络和多核芯片，我们可以设计系统，实现高吞吐量和低延迟和强大的保护和资源效率。它也表明，通过分离的小子集的性能关键/输出功能，从其余的内核，我们可以从根本上不同的输入/输出系统实现大的性能增益，同时保持与一个现代化的操作系统所提供的大量的接口和服务的兼容性。


###原理

保护：由于多个服务通常共享公共和私人的数据中心服务器，所以需要在应用程序之间进行隔离。使用内核，或基于超级指令的网络协议栈在很大程度上解决了上述问题。
一个值得信赖的网络堆栈可以防火墙应用程序，强制访问控制列表（ACL），以及实施带宽计量限制器等。
资源效率：数据中心应用负载由于昼夜模式用户流量显著变化，理想的是，每一个服务节点将使用最少资源（核心，存储器，或IOPS）需要满足包速率和尾部等待时间要求。
剩余的服务器资源可以分配给其它应用或放置成用于能量低功率模式来提高效率。现有的操作系统能够支持这样的资源使用策略。
然而如今的商业操作系统一直在非常不同的硬件设计的假设下面设计的。内核调度，网络API和网络协议栈是在多个应用的假设下设计的，它们共享单个处理核，包到达间隔时间比中断延迟和系统调用高出许多倍。
缓冲和同步的开销需要灵活，精细的调度的应用程序支持,以提高核心CPU和内存系统的开销，这限制了吞吐量。随着服务之间的请求的数据中心应用层通常由小分组，共同NIC硬件优化，如TCP分段和接收端的聚结，有数据包率的边际影响。
由于商业内核的网络栈不能利用硬件资源丰富性的优势，提出了若干替代方法，比如用户空间网络堆栈，Alternatives to TCP，Alternatives to POSIX API，OS enhancements。

<img src="http://7xrmn9.com1.z0.glb.clouddn.com/aos11.png" style="width: 50%; height: 50%"/>​
 
在现代服务器硬件资源的财富应该让数据中心应用的低延迟和高丢包率。一个典型的服务器包括一个或两个处理器插座，每八个或更多的多线程的内核和多通道、高速DRAM和PCIe设备。
固态硬盘和基于闪存的PCIe也越来越受欢迎。因为网络，10个GbE NIC和交换机被广泛部署在数据中心，40 GbE和100 GbE技术对周围的角落。对硬件线程数和10 GbE NIC组合应允许15m包/秒
的速率最小尺寸的包。我们也应该达到10–20µ的往返延迟了3µ的延迟在一对10 GbE NIC，一至五开关通道穿过几百纳秒每延迟，和数据中心内的100米距离500ns的传播延迟。不幸的是，
商品操作系统已经设计了非常不同的硬件假设。内核调度，网络API，网络堆栈已被设计的假设下，多个应用程序共享一个单一的处理核心和数据包到达时间比延迟中断高许多倍，系统调用。
因此，这样的操作系统的延迟和吞吐量，有利于细粒度的资源调度。中断联合（用于减少处理开销），排队时延，由于设备驱动程序处理的时间间隔，中间缓冲的使用，和CPU调度延迟经常总
计达几百µ远程请求的延迟。缓冲和同步的开销，以支持灵活的，细粒度的应用程序的内核增加处理器和内存系统的开销，限制吞吐量。作为数据中心的应用服务层之间的请求通常包括小包，
普通网卡硬件的优化，如TCP分割和接收端合并，对数据包传输率的有很大的边际影响。

由于在商品内核的网络栈不能充分利用硬件资源的丰富，一些替代的方法已被建议。每个备选地址的一个子集，但不是所有的，对数据中心的应用需求。用户空间的网络协议栈：
系统如openonload ，MTCP，和沙尘暴在用户空间运行整个网络堆栈以消除核交叉开销和优化分组处理不产生核改性阳离子的复杂性。然而，
仍然有数据包率和延迟之间的权衡。例如，MTCP使用专用线程进行通信的TCP协议栈，在较粗粒度线程与应用。这种激进的分批分期摊销的费用在高延迟切换开销。
它也使资源共享的复杂性，因为网络堆栈必须使用大量的硬件线程，无论实际负载。更重要的是，当网络被提升到用户空间和应用程序的错误可以破坏网络堆栈时，安全性的权衡出现了。
例如，攻击者可以发送原始数据包（通常需要根权限的能力），利用网络协议的弱点和影响其他服务。这是很难执行的任何安全或计量超出直接通过网卡硬件支持的政策。选择TCP：
除了内核旁路，一些低延迟对象存储依赖RDMA卸载专用InfiniBand主机通道适配器。RDMA可以减少延迟，但需要专门的适配器是在连接的两端。使用商品以太网网络，
facebook的缓存部署使用UDP连接的可扩展性，避免局限性。虽然UDP在内核中运行，可靠的通信和拥塞管理委托的应用。POSIX API的替代品：megapipe取代POSIX API实现轻量级的插座和内存中的指令环。
这减少了一些软件开销和增加包率，但保留所有其他的挑战，使用现有的，基于内核的网络协议栈。操作系统增强：调整内核为基础的栈提供了增量的好处，优越的易于部署。
Linux 重用端口允许多线程应用程序在并行接受传入的连接。affinity accept以确保所有处理的网络流量是关联到相同的核心[ 49 ]减少开销。最近的Linux内核支持一个繁忙的轮询驱动模式行业增加CPU使用率降低延迟，
但它是不兼容的实现。

对于IX设计的方法，许多中间件dataplanes采用的设计原则与传统的操作系统不同。商业操作系统从应用程序本身解耦协议处理，以便提供调度和流量控制的灵活性。
例如，内核依赖于设备和软中断应用在程序协议处理的上下文中切换。同样，内核网络堆栈将产生TCP ACK并且滑动接受窗口，即使在应用程序不消耗窗口数据，到了最多的程度。
其次，中间件dataplanes为了免同步操作优化，这使得在许多内核进行了很好地扩展。网络流量分布在不同的队列中，通过常流量的hash和包处理不需要同步的通常情况。
作为对比，传统操作系统往往在很大程度上依赖交通的连贯性并构造锁等同步形式。
控制和数据链路的分隔和保护，IX分隔内核的控制功能，负责从数据平面资源配置，调度，和监测，它运行网络协议栈和应用逻辑。
运行自适应分批完成，IX 数据平面运行完成接收和传送数据包的所有阶段，使得内核态的协议处理和用户态的应用程序逻辑在定义好的变换点很好地交织。
有明确的流量控制的native,zero-copy api，我们不公开或者模拟网络的POSIX API。相反，数据面内核和应用程序在存储中使用消息来协调过渡点的通信。
一致网络流，免除同步处理，我们使用了带有接收端缩放多队列的NICs网卡提供了一致网络流到来通路的哈希，并且分导到不同的硬件队列中。
dataplane不同于典型的内核，他特指高性能网络IO，只运行单个程序，类是一个库操作系统但是内存是分割单独，此外 dataplan还提供很多熟悉的内核层次服务。

###设计方法

首先两个需求要求微秒的延迟和高数据率，对于数据中心的应用程序不是独特的。这些要求已经被处理通过整合网络协议栈和应用在单一的数据层次中在中间件如防火墙，负载均衡器的设计处理，软件路由器。

<img src="https://github.com/dengguang2012/paper-Reading-Report/blob/master/illustraction/27.jpg" style="width: 50%; height: 50%"/>

许多中间件数据层次采用不同于传统的操作系统设计原则。首先，它们运行每个数据包来完成。所有的网络协议和应用程序处理的数据包之前，移动到下一个数据包，
并应用逻辑通常是和网络堆栈交织在一起的而不分离。相比之下，商品OS分离应用协议处理，以提供调度和流量控制的灵活性。例如，内核依赖于设备和软中断从应用程序到协议处理的上下文切换。
同样，内核网络协议栈的TCP ACK将产生滑动，其接收窗口即使应用不消耗数据，在一定程度上。其次，代理dataplanes优化同步操作以规模以及多芯。网络流量分布到不同的队列，
通过流量一致的哈希和共同的情况下的数据包处理不需要同步或连贯性之间的流量。相比之下，商品操作系统很大程度上依赖于相干交通结构使同步锁和其他形式的频繁使用。
控制和数据层次的分离和保护，自适应的批处理来完成运行，使用显式流量控制的本地、零拷贝API，流量一致，自由同步处理。

对于IX dataplane更详细的内容，它不同于一个典型的内核，它是专门为高性能网络输入/输出，并运行只有一个单一的应用程序，类似于一个库系统，但有内存隔离。然而，
我们的dataplane仍然提供了许多熟悉的内核级进行时间服务。对于内存管理，作者接受一些内部的内存碎片，以降低复杂性和提高效率。所有热路径数据对象都是从硬件线程内存池中分配的。
每个内存池的结构数组的大小相同的物体，在配置页面大小的块。免费对象是一个简单的免费列表跟踪，并分配函数内联直接调用函数。mbufs，对网络数据包的存储对象，存储为记账数据和MTU大小的缓冲区相邻的块，和用于接收和发送数据包。

数据层面还管理其自己的虚拟地址转换，通过嵌套分页支持。相反，现代的操作系统，它使用专用的大页（2MB）。大网页是由于他们的减少地址转换开销和相对丰富的物理内存资源，在现代服务器。dataplane仅维护一个单一的地址空间；内核页被主管位保护。我们故意选择不支持插拔内存以避免增加性能的变化。
提供分层时间车轮实施管理网络超时，如TCP重传。这是优化的一般情况，大多数定时器到期在它们被取消之前，支持极高分辨率的超时，低至16µs，这已被证明是提高TCP添头拥塞期间的表现

数据层API和操作

一个应用程序的弹性螺纹与IX dataplane通过三次异步、非阻塞机制：他们的问题进行批处理系统调用的dataplane；他们消费的事件条件的dataplane产生的；和他们有直接的，但安全访问（mbufs）含有输入载荷。后者允许零拷贝访问传入的网络流量。应用程序可以继续mbufs直到它要求dataplane释放他们通过recv做批处理系统调用。
批处理系统调用和事件的条件下，通过共享存储器阵列，分别由用户和内核操作。IX提供unbatched系统调用（运行IO）产生控制内核和初始化启动新的运行周期的终结。作为周期的一部分，
内核改写成批系统调用请求与对应的返回码数组和数组填充事件条件。在建立连接的用户提供了一个不透明的价值使高效的面向用户状态查询,IX与POSIX套接字，它直接揭示了流量控制条件下的应用。
sendv系统调用不返回的字节数缓冲。相反，它返回的字节，接受和发送的TCP协议栈的数目，通过正确的TCP滑动窗口操作的约束。当接收到的字节，发送的事件条件通知应用程序，它是可能的发送更多的数据。
因此，发送窗口大小的策略是完全由应用程序来决定的。相比之下，传统的操作系统缓冲区发送数据超出原TCP流量控制的政策约束和内核。我们建立了一个用户级库，称为libix，它抽象了我们的底层API的复杂性。
它为传统的应用程序提供了一个兼容的编程模型，并大大简化了新应用程序的开发。libix目前包括一个非常类似的界面，libevent和非阻塞POSIX套接字操作。它还包括zerocopy读和写是更有效的操作界面，
在需要修改现有的应用程序的费用。libix自动合并多个写请求为单sendv系统调用每个配料在轮。这提高了局部性，简化了错误处理，并确保正确的行为，因为它保留了数据流的顺序，即使发送失败。
合并也有利于传输流量控制因为我们可以使用传输载体（参数sendv）跟踪输出数据的缓冲区，如果有必要，重新写时，发送窗口有更多的可用空间，通过发送事件情况通报。我们的缓冲区大小的政策是目前非常基本的，
我们执行一个最大的等待发送字节的限制，但我们计划，使这更动态的未来。

对于多核的扩展，IX dataplane是多核的可扩展性进行了优化，弹性线在一般情况下，同步和相干免费的方式操作。这是一个更强大的要求，比锁自由同步，这需要昂贵的原子指令，
即使在一个单一的线程是一个特定的数据结构的主要消费。这是通过一组有意识的设计和实施的权衡是可能的。

对于安全模型，IX API和实现应用程序代码和网络处理协议栈之间的协作流程控制模型。
不同的用户级别的堆栈，凡申请是可信的正确的网络行为，IX 保护模式对应用的几个假设。一个恶意的或行为不端的应用只能伤害自己。它不能损坏网络堆栈或影响其他应用程序。
所有应用程序代码在IX在用户模式下运行的，而在保护ring0运行dataplane代码。应用程序不能访问dataplane内存，除了只读消息缓冲区。没有序列的批处理系统调用或其他用户级措施可以违背正确遵守TCP和其他网络号规格。
此外，该dataplane可以用来执行网络安全策略，如防火墙和访问控制列表。IX安全模型是传统的基于内核的网络协议栈为强，这一特点是从最近提出的用户级栈消失。


###测试评估和相关工作

测试使用了IX和Linux  mTCP做了对比，带宽延迟都要更优秀，究其原因是把网络工作栈是现在一个保护的操作系统内核中，并能够对于很多benchmark提供传输线级别性能。但是IX对于动态运行时没有做优化。
相关工作比如Hardware Virtualization Arrakis使用硬件虚拟化分隔IO数据层和控制层，对控制层，网络堆栈和应用程序进行分离
